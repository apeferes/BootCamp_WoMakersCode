{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipelines\n",
    "\n",
    "## Trabalhando com pipelines de dados\n",
    "\n",
    "# Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é?\n",
    "\n",
    "### Automação\n",
    "\n",
    "- Coleta\n",
    "- Transformação\n",
    "- Carregamento / Armazenamento\n",
    "- Monitoramento\n",
    "\n",
    "Pipeline de dados em português, é um sistema que facilita a coleta, processamento e movimentação de dados de uma fonte para um destino específico.Uma pipeline de dados, em termos gerais, é uma série de processos ou etapas interligadas que são usadas para mover e transformar dados de um ponto de origem para um destino específico. As pipelines de dados são comumente usadas em projetos de ciência de dados, engenharia de dados, análise de dados e em geral em ambientes onde é necessário lidar com grandes volumes de dados de maneira eficiente e automatizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qual a diferença entre data wrangling e pipelines?\n",
    "\n",
    "### Data Wrangling\n",
    "mise en place\n",
    "- A manipulação de dados - torna-los adequados a receita\n",
    "\n",
    "Suponha que você tenha um conjunto de dados com informações sobre clientes, mas as datas estão em um formato bagunçado. Data wrangling seria o processo de organizar essas datas para que elas possam ser facilmente compreendidas e utilizadas, como convertê-las para um formato padrão.\n",
    "\n",
    "A manipulação de dados é como organizar e preparar esses ingredientes para torná-los adequados à receita. Isso envolve limpar os ingredientes, cortá-los do jeito certo, misturar quando necessário, e garantir que tudo esteja pronto para ser usado.\n",
    "\n",
    "### Data Papiline\n",
    "Plantio e colheita\n",
    "- Coletando / Extraindo\n",
    "\n",
    "Cozimento\n",
    "- Transformando\n",
    "\n",
    "Serviço\n",
    "- Carregamento\n",
    "\n",
    "Vamos dizer que você tenha dados de vendas em diferentes lojas e deseje analisar o desempenho total. A ETL envolveria extrair os dados de cada loja, transformá-los para garantir consistência (por exemplo, unificar formatos de datas, moedas), e carregá-los em um único local para análise.\n",
    "\n",
    "A ETL é como o processo de preparação de uma refeição completa, desde a escolha dos ingredientes (extração), o preparo (transformação) até colocar a comida na mesa (carregamento). No contexto de dados, ETL é o processo de mover dados de um lugar para outro, transformá-los para atender às necessidades específicas e, finalmente, carregá-los em um local onde possam ser utilizados. manipulação de dados é como organizar e preparar esses ingredientes para torná-los adequados à receita. Isso envolve limpar os ingredientes, cortá-los do jeito certo, misturar quando necessário, e garantir que tudo esteja pronto para ser usado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta\n",
    "\n",
    "### Coletando dados\n",
    "\n",
    "Coletamos dados de diferentes fontes e é necessário checá-las antes. Normalmente em um projeto nós temos a fonte de dados disponível, mas em outros ainda temos que buscar essa informação e checar a qualidade desse dado, como a fonte.\n",
    "\n",
    "\n",
    "Exploração de Dados: Verificação dos tipos de dados das características, valores únicos e descrição dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação\n",
    "\n",
    "Separamos limpeza e transformação em data wrangling, agora vamos por ambos no mesmo momento.\n",
    "\n",
    "### O que é?\n",
    "\n",
    "Processando e transformando os dados de acordo com as necessidades do projeto, incluindo limpeza, normalização, agregação, etc.\n",
    "\n",
    "\n",
    "Imagine que você está preparando uma receita que requer diferentes temperos para realçar o sabor do prato. A transformação de dados, nesse contexto, seria como ajustar a quantidade, a mistura e a preparação desses temperos para obter o sabor desejado.\n",
    "\n",
    "\n",
    "- Remoção de Impurezas (Limpeza de Dados):\n",
    "Assim como você lavaria e removeria qualquer sujeira dos legumes antes de cortá-los, na limpeza de dados, você removeria valores nulos, duplicatas ou outliers que possam afetar a qualidade dos dados.\n",
    "\n",
    "- Combinação de Sabores (Feature Engineering):\n",
    "Misturar diferentes ervas e especiarias para criar uma mistura exclusiva que complementa o prato é como realizar feature engineering nos dados. Você está criando novas características (sabores) que agregam valor ao prato final.\n",
    "\n",
    "- Mudança na Forma (Transformação de Dados):\n",
    "Se a receita original pede alho picado, mas você prefere alho em pó, fazer essa substituição é uma forma de transformação de dados. Você está alterando a forma dos dados (alho) para melhor atender às suas preferências ou requisitos específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de Limpeza\n",
    "\n",
    "Processando e transformando os dados de acordo com as necessidades do projeto, incluindo limpeza, normalização, agregação, etc.\n",
    "\n",
    "\n",
    "Neste exemplo: utilizamos fillna para preencher os valores nulos na coluna 'Preco' com a média dos valores existentes.Removemos outliers definindo um limite superior (nesse caso, 1000) para a coluna 'Preco'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Produto': ['A', 'B', 'C', 'D', 'E'],\n",
    "        'Preco': [100.0, 120.0, None, 150.0, 5000.0]} # Incluindo um valor nulo e um outlier\n",
    "df = pd. DataFrame(data)\n",
    "\n",
    "# Exibindo o DataFrame original\n",
    "print (\"DataFrame Original:\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza de dados: tratando valores nulos e outliers na coluna 'Preco\"\n",
    "# Substituindo valores nulos pela média e removendo outliers (valores acima de 1000) df ['Preco']. fillna(df[ 'Preco']. mean(), inplace-True)\n",
    "df = df[df['Preco'] < 1000]\n",
    "\n",
    "# Exibindo o DataFrame após a limpeza print(\"\\nDataFrame após Limpeza:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de transformação\n",
    "\n",
    "Processando e transformando os dados de acordo com as necessidades do projeto, incluindo limpeza, normalização, agregação, etc.\n",
    "\n",
    "\n",
    "Neste exemplo simples, a transformação envolve a conversão da coluna 'Data' para o formato de data, a criação de uma nova coluna 'DiaDaSemana' e a aplicação de One-Hot Encoding a essa coluna. Essas transformações são específicas para o contexto do conjunto de dados e das análises desejadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que 'df' seja o DataFrame com os dados de vendas\n",
    "df['Data'] = pd.to_datetime(df['Data']) # Convertendo para o formato de data\n",
    "\n",
    "# Criando uma nova coluna com o dia da semana\n",
    "df['DiaDaSemana'] = df['Data'].dt.day_name()\n",
    "\n",
    "# One-Hot Encoding para a coluna 'DiaDaSemana'\n",
    "df = pd.get_dummies(df, columns=['DiaDaSemana'])\n",
    "\n",
    "# Exibindo as primeiras linhas do DataFrame após a transformação\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento / Armazenamento\n",
    "\n",
    "### O que é?\n",
    "\n",
    "Armazenando os dados transformados em um local de destino, como um banco de dados, um data warehouse, ou mesmo um arquivo.\n",
    "\n",
    "\n",
    "Imagine que você está preparando uma receita especial e precisa de diferentes ingredientes que estão em diferentes lugares. O carregamento de dados seria como ir até a despensa, à geladeira e à despensa novamente para pegar todos os ingredientes necessários.\n",
    "\n",
    "\n",
    "- Carregamento dos Dados: Você, como chef de dados, precisa coletar todos esses ingredientes de suas respectivas fontes. Você vai até a geladeira para pegar os ovos, à despensa para a farinha e ao armário para o açúcar.\n",
    "\n",
    "- Destino (Prato Final): Depois de coletar todos os ingredientes necessários (carregamento de dados), você os reúne na bancada para\n",
    "começar a preparar seu prato final (resultado desejado).\n",
    "\n",
    "- Uso dos Ingredientes (Análise/Relatórios):\n",
    "Agora que você tem todos os ingredientes no local de preparo, você pode começar a usá-los para criar sua obra-prima culinária (análises, relatórios ou outras ações que você deseja realizar com os dados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento\n",
    "\n",
    "CSV + API + TABELA EXCEL\n",
    "\n",
    "= TRANSFORMAÇÃO\n",
    "\n",
    "= ARMAZENAMENTO DE DADOS PROCESSADOS\n",
    "\n",
    "\n",
    "OU\n",
    "\n",
    "\n",
    "CSV + API + TABELA EXCEL\n",
    "\n",
    "= ARMAZENAMENTO DE DADOS PROCESSADOS\n",
    "\n",
    "= TRANSFORMAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Suponhamos que temos dados em um arquivo CSV, em um banco de dados SQL e em uma API.\n",
    "\n",
    "# Carregando dados de um arquivo CSV\n",
    "df_csv = pd. read_csv('dados_csv.csv')\n",
    "\n",
    "# Conectando ao banco de dados SQL (SQLite neste exemplo)\n",
    "engine = create_engine('sqlite:///banco_sql.db)\n",
    "query_sql - 'SELECT * FROM tabela_sql'\n",
    "df_sql = pd.read_sql_query(query_sql, engine)\n",
    "\n",
    "# Supondo uma API fictícia (você precisa da URL real da API e as credenciais, se necessário)\n",
    "api_url - 'https://api.exemplo.com/dados'\n",
    "df_api = pd.read_json(api_url)\n",
    "\n",
    "# Agora, temos os dados de diferentes fontes. Vamos transformá-los.\n",
    "\n",
    "# Transformação: Por exemplo, adicionar uma nova coluna aos DataFrames\n",
    "df_csv['Nova_Coluna'] = df_csv['Coluna_A'] + df_csv['Coluna_B'] \n",
    "df_sql['Nova_Coluna'] = df_sql['Coluna_C'] * 2\n",
    "df_api['Nova_Coluna'] = df_api['Coluna_X'] - df_api['Coluna_Y']\n",
    "\n",
    "# Agora, vamos concatenar (ou combinar) os DataFrames em um único DataFrame\n",
    "\n",
    "df_final = pd.concat([df_csv, df_sql, df_api], ignore_index=True)\n",
    "\n",
    "# Por fim, vamos carregar o DataFrame final em um arquivo CSV ou em um banco de dados.\n",
    "\n",
    "# Salvando em um arquivo CSV\n",
    "df_final.to_csv('dados_final.csv', index=False)\n",
    "\n",
    "# Salvando no mesmo banco de dados SQL\n",
    "df_final.to_sql('tabela_final', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoramento\n",
    "\n",
    "### O que é?\n",
    "\n",
    "Monitorar o desempenho da pipeline, gerenciar falhas e lidar com situações excecionais são partes críticas da administração de uma pipeline de dados.\n",
    "\n",
    "Imagine que você está preparando uma receita especial e precisa de diferentes ingredientes que estão em diferentes lugares. O carregamento de dados seria como ir até a despensa, à geladeira e à despensa novamente para pegar todos os ingredientes necessários.\n",
    "\n",
    "\n",
    "- Tempo de Cozimento (Monitoramento de Desempenho): Ao preparar diferentes pratos, você monitora o tempo de cozimento para garantir que cada componente seja cozido corretamente. Na pipeline de dados, isso seria como monitorar o tempo que cada etapa leva para ser concluída.\n",
    "\n",
    "- Degustação (Monitoramento de Qualidade): Você faz degustações ao longo do processo para ajustar o tempero e garantir que o sabor esteja no ponto certo. No contexto de uma pipeline de dados, seria equivalente a verificar a qualidade dos dados em cada etapa. \n",
    "\n",
    "- Controle de Inventário (Monitoramento de Recursos): Você verifica se há ingredientes suficientes e se tudo está em ordem no inventário. Da mesma forma, em uma pipeline de dados, você monitoraria o uso de recursos, como capacidade de armazenamento e poder de processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes tecnologias para monitoramento de dados\n",
    "\n",
    "- Apache Airflow\n",
    "- Prometheus\n",
    "- Grafana\n",
    "- Datadog\n",
    "- Azyure Application Insights\n",
    "- Python usando a biblioteca loggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configurando o logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Função de Extração\n",
    "def extract_data (file_path):\n",
    "    logging.info(\"Iniciando extração de dados...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    logging.info(\"Extração de dados concluída.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automação\n",
    "\n",
    "### O que é?\n",
    "\n",
    "Automação em uma pipeline de dados refere-se à capacidade de realizar tarefas e processos de forma programada e sem intervenção manual. Isso é essencial para garantir eficiência, consistência e confiabilidade em todo o fluxo de dados. Aqui estão alguns aspectos-chave da automação em uma pipeline de dados:\n",
    "\n",
    "- Agendamento de Tarefas\n",
    "- Orquestração de Fluxo de Trabalho\n",
    "- Gestão de Dependências\n",
    "- Monitoramento e Notificação\n",
    "- Tratamento de Erros e Retentativas\n",
    "- Gerenciamento de Configuração\n",
    "- Atualizações Automáticas\n",
    "- Integração Contínua e Implantação Contínua (CI/CD)\n",
    "- Escalonamento Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import wordcloud\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://portaldatransparencia.gov.br/api-de-dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"chave-api-dados\" : \"0000000000000000\"}\n",
    "\n",
    "# vou usar outra fonte de chave abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o arquivo CSV\n",
    "csv_path = '/Users/anapaulaferes/Desktop/Curso/Tokens e Chaves/govbr.csv'\n",
    "\n",
    "# Carregar o arquivo CSV para um DataFrame pandas\n",
    "df_token = pd.read_csv(csv_path)\n",
    "\n",
    "# Obter o token da coluna 'value'\n",
    "token_value = df_token.loc[0, 'value']\n",
    "\n",
    "# Definir os cabeçalhos da solicitação com o token\n",
    "headers = {\"chave-api-dados\": token_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL da API\n",
    "url = 'https://api.portaldatransparencia.gov.br/api-de-dados/emendas?ano=2022&pagina=1'\n",
    "\n",
    "# Fazendo a solicitação GET com os cabeçalhos\n",
    "resposta_ = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificando se a solicitação foi bem sucedida\n",
    "if resposta_.status_code == 200:\n",
    "    dados = resposta_.json()\n",
    "    print(dados)  # Exibindo os dados da resposta\n",
    "else:\n",
    "    print(\"Erro ao fazer a solicitação:\", resposta.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta_.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resposta_.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.portaldatransparencia.gov.br/api-de-dados/emendas?ano=2022&pagina='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ementas página até 408\n",
    "\n",
    "list_urls = [url + str(i) for i in range (1, 409)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passando a lista para um array\n",
    "arr_urls = np.asarray(list_urls)\n",
    "arr_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_run = np.vectorize(run_request)\n",
    "\n",
    "arr_responses = vec_run(arr_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_responses = arr_responses.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arr_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar todas essas listas em uma só"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.concatenate(arr_responses).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_request(url_):\n",
    "    resposta = requests.get(url, headers=headers)\n",
    "    return resposta.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = np.concatenate(arr_responses).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['valorEmpenhado'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['valorEmpenhado'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_area = df[['funcao', 'valorEmpenhado']]\n",
    "\n",
    "df_area['valorEmpenhado'] = df_area['valorEmpenhado'].str.replace('.', '').str.replace(',', '.').astype(np.float32)\n",
    "df_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_area.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_area = df_area.groupby('funcao').sum('valorEmpenhado')\n",
    "agg_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_area = agg_area['valorEmpenhado']/df_area['valorEmpenhado'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_area = agg_area.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_area.to_csv('distribuicao_empenho_area_2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um metodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_request(url_):\n",
    "    resposta = requests.get(url_, headers=headers)\n",
    "    return resposta.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coletar_dados(url):\n",
    "    logging.info('Pegou url para a criação das diferentes URLs criando uma para cada página')\n",
    "    url = 'https://api.portaldatransparencia.gov.br/api-de-dados/emendas?ano=2022&pagina='\n",
    "    list_urls = [url + str(i) for i in range (1, 409)]\n",
    "    arr_urls = np.asarray(list_urls)\n",
    "\n",
    "    vec_run = np.vectorize(run_request)\n",
    "    logging.info('Fazendo uma requisição para cada URL')\n",
    "    arr_responses = vec_run(arr_urls)\n",
    "    logging.info('Requisições concluídas')\n",
    "    arr_responses = arr_responses.tolist()\n",
    "    resposta = np.concatenate(arr_responses).tolist()\n",
    "    df = pd.DataFrame(resposta)\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_dado(dado):\n",
    "    logging.info('Criando dataframe')\n",
    "    df = pd.DataFrame(dado)\n",
    "\n",
    "    logging.info('Selecionando colunas')\n",
    "    df_area = df[['funcao', 'valorEmpenhado']]\n",
    "\n",
    "    logging.info('Transformando dado string -> float')\n",
    "    df_area['valorEmpenhado'] = df_area['valorEmpenhado'].str.replace('.', '').str.replace(',', '.').astype(np.float32)\n",
    "    agg_area = df_area.groupby('funcao').sum('valorEmpenhado')\n",
    "    \n",
    "    logging.info('Transformando dados para retirar proporção em porcentagem')\n",
    "    agg_area = agg_area['valorEmpenhado']/df_area['valorEmpenhado'].sum()\n",
    "\n",
    "    agg_area = agg_area.reset_index()\n",
    "    return agg_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga(dado):\n",
    "    dado.to_csv('distribuicao_empenho_area_2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl():\n",
    "    url = 'https://api.portaldatransparencia.gov.br/api-de-dados/emendas?ano=2022&pagina='\n",
    "    dado = coletar_dados(url)\n",
    "    dado = transformar_dado(dado)\n",
    "    carga(dado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl()\n",
    "\n",
    "# VER RODANDO NO ARQUIVO'Sklean Pipelines.py'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
